{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa01cce",
   "metadata": {
    "papermill": {
     "duration": 0.006894,
     "end_time": "2025-03-07T20:45:54.615882",
     "exception": false,
     "start_time": "2025-03-07T20:45:54.608988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Home Credit Default Risk - Machine Learning Project  \n",
    "\n",
    "## Project Overview  \n",
    "This project aims to predict **loan default risk** using historical credit data provided by the **Home Credit dataset**.  \n",
    "By analyzing multiple financial datasets from past loan applications, we extract insights to improve risk assessment and minimize losses for lenders.  \n",
    "While this model is trained specifically on Home Credit‚Äôs dataset, the process‚Äîdata collection, preprocessing, feature engineering, and modeling‚Äîcan be adapted to other financial institutions. \n",
    "\n",
    "## Live Application Deployment  \n",
    "This project is also deployed as an **interactive Angular + Flask application**, allowing users to observe real-time model inference.  \n",
    "üîó **Try it here:** [Live Loan Default Predictor](https://ai.fullstackista.com/ai-loan-default-predictor/)  \n",
    "\n",
    "### Key Steps in the Project  \n",
    "1. **Understanding the Problem** ‚Äì Define the objective: predict loan default risk using Home Credit data.  \n",
    "2. **Data Processing & Feature Engineering** ‚Äì Process multiple datasets, clean missing values, extract features, and aggregate information.  \n",
    "3. **Exploratory Data Analysis (EDA)** ‚Äì Identify trends, correlations, and risk factors in loan applications.  \n",
    "4. **Merging Datasets** ‚Äì Integrate primary (`application_train.csv`) and secondary datasets (e.g., `bureau.csv`, `credit_card_balance.csv`) for a unified view.  \n",
    "5. **Model Training & Hyperparameter Tuning** ‚Äì Train and optimize models (e.g., LightGBM) for predictive performance.  \n",
    "6. **Model Evaluation** ‚Äì Validate performance using metrics such as AUC-ROC.  \n",
    "7. **Final Prediction** ‚Äì Apply the trained model to `application_test.csv` and generate predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b287852",
   "metadata": {
    "papermill": {
     "duration": 0.005907,
     "end_time": "2025-03-07T20:45:54.628167",
     "exception": false,
     "start_time": "2025-03-07T20:45:54.622260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## About This Notebook  \n",
    "\n",
    "This notebook prepares the **test dataset (`application_test.csv`)** for **predictions** using the trained model.  \n",
    "\n",
    "### Key Steps:\n",
    "- **Load & Merge Datasets** ‚Äì Processed datasets are loaded and merged in the same way as during model training.  \n",
    "- **Generate Predictions** ‚Äì The trained LightGBM model is applied to the test data.  \n",
    "- **Create Prediction File** ‚Äì While real-time model deployment is often preferred in production (as demonstrated in my [Angular/Flask-based deployed app](https://ai.fullstackista.com/ai-loan-default-predictor/)), structured output files are still used in certain batch-processing scenarios, such as risk assessment and regulatory reporting in financial applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902f9a7",
   "metadata": {
    "papermill": {
     "duration": 0.005847,
     "end_time": "2025-03-07T20:45:54.640173",
     "exception": false,
     "start_time": "2025-03-07T20:45:54.634326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Project Notebooks  \n",
    "\n",
    "### Main Dataset and Model Training  \n",
    "- [1. Application Train (Main Dataset)](./01_application_train.ipynb)\n",
    "- [2. Model Training and Final Pipeline](./02_model_training_pipeline.ipynb)  \n",
    "\n",
    "### Secondary Datasets Processing  \n",
    "- [3. Bureau Data](./03_bureau_data.ipynb)  \n",
    "- [4. Bureau Balance Data](./04_bureau_balance.ipynb)  \n",
    "- [5. Credit Card Balance](./05_credit_card_balance.ipynb)  \n",
    "- [6. Previous Applications](./06_previous_applications.ipynb)  \n",
    "- [7. POS Cash Balance](./07_pos_cash_balance.ipynb)  \n",
    "- [8. Installments Payments](./08_installments_payments.ipynb)  \n",
    "\n",
    "### Final Prediction  \n",
    "- [9. Model Predictions on Test Data](./09_model_predictions.ipynb) _(Current Notebook)_\n",
    "- [10. Application Test Data Processing](./10_application_test_processing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8937465",
   "metadata": {
    "papermill": {
     "duration": 0.005771,
     "end_time": "2025-03-07T20:45:54.652093",
     "exception": false,
     "start_time": "2025-03-07T20:45:54.646322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Predictions (All Datasets)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82abe95",
   "metadata": {
    "papermill": {
     "duration": 0.005792,
     "end_time": "2025-03-07T20:45:54.663902",
     "exception": false,
     "start_time": "2025-03-07T20:45:54.658110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Load All Processed Datasets  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20135dae",
   "metadata": {
    "papermill": {
     "duration": 0.00574,
     "end_time": "2025-03-07T20:45:54.675722",
     "exception": false,
     "start_time": "2025-03-07T20:45:54.669982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1 Load Required Libraries  \n",
    "Import necessary Python libraries for **data processing, loading the trained model, and making predictions.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0706c3b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:45:54.689192Z",
     "iopub.status.busy": "2025-03-07T20:45:54.688788Z",
     "iopub.status.idle": "2025-03-07T20:46:00.372969Z",
     "shell.execute_reply": "2025-03-07T20:46:00.371782Z"
    },
    "papermill": {
     "duration": 5.693576,
     "end_time": "2025-03-07T20:46:00.375276",
     "exception": false,
     "start_time": "2025-03-07T20:45:54.681700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb719c",
   "metadata": {
    "papermill": {
     "duration": 0.005991,
     "end_time": "2025-03-07T20:46:00.387767",
     "exception": false,
     "start_time": "2025-03-07T20:46:00.381776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2 Define Dataset Path  \n",
    "Set the file path where the processed **test dataset** is stored.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5c458c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:00.401776Z",
     "iopub.status.busy": "2025-03-07T20:46:00.401003Z",
     "iopub.status.idle": "2025-03-07T20:46:00.405628Z",
     "shell.execute_reply": "2025-03-07T20:46:00.404474Z"
    },
    "papermill": {
     "duration": 0.013465,
     "end_time": "2025-03-07T20:46:00.407349",
     "exception": false,
     "start_time": "2025-03-07T20:46:00.393884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "DATASET_PATH = \"/kaggle/input/home-credit-processed-data-and-model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c867d1",
   "metadata": {
    "papermill": {
     "duration": 0.005795,
     "end_time": "2025-03-07T20:46:00.419560",
     "exception": false,
     "start_time": "2025-03-07T20:46:00.413765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.3 Load Processed Datasets  \n",
    "Load all preprocessed datasets from the specified directory.  \n",
    "These datasets include the main **test dataset** and various secondary datasets that provide additional financial and credit history details for loan applicants.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4305e686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:00.433462Z",
     "iopub.status.busy": "2025-03-07T20:46:00.433064Z",
     "iopub.status.idle": "2025-03-07T20:46:05.906443Z",
     "shell.execute_reply": "2025-03-07T20:46:05.905188Z"
    },
    "papermill": {
     "duration": 5.482693,
     "end_time": "2025-03-07T20:46:05.908319",
     "exception": false,
     "start_time": "2025-03-07T20:46:00.425626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All datasets loaded successfully from Kaggle dataset\n"
     ]
    }
   ],
   "source": [
    "# Load data from processed dataset\n",
    "df_application_test_processed = pd.read_pickle(DATASET_PATH + \"application_test_processed.pkl\")\n",
    "df_bureau_aggregated = pd.read_pickle(DATASET_PATH + \"bureau_aggregated.pkl\")\n",
    "df_bureau_balance_aggregated_with_curr_final = pd.read_pickle(DATASET_PATH + \"bureau_balance_aggregated_with_curr_final.pkl\")\n",
    "df_pos_cash_balance_aggregated = pd.read_pickle(DATASET_PATH + \"pos_cash_balance_aggregated.pkl\")\n",
    "df_credit_card_balance_aggregated = pd.read_pickle(DATASET_PATH + \"credit_card_balance_aggregated.pkl\")\n",
    "df_previous_application_aggregated = pd.read_pickle(DATASET_PATH + \"previous_application_aggregated.pkl\")\n",
    "df_installments_payments_aggregated = pd.read_pickle(DATASET_PATH + \"installments_payments_aggregated.pkl\")\n",
    "\n",
    "# Confirm successful loading\n",
    "print(\"‚úÖ All datasets loaded successfully from Kaggle dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2159a",
   "metadata": {
    "papermill": {
     "duration": 0.006324,
     "end_time": "2025-03-07T20:46:05.921277",
     "exception": false,
     "start_time": "2025-03-07T20:46:05.914953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.4 Check Dataset Shapes  \n",
    "Display the number of rows and columns in each dataset to ensure proper loading.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "107db55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:05.935312Z",
     "iopub.status.busy": "2025-03-07T20:46:05.934915Z",
     "iopub.status.idle": "2025-03-07T20:46:05.942563Z",
     "shell.execute_reply": "2025-03-07T20:46:05.941386Z"
    },
    "papermill": {
     "duration": 0.01659,
     "end_time": "2025-03-07T20:46:05.944239",
     "exception": false,
     "start_time": "2025-03-07T20:46:05.927649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shapes:\n",
      "Application Test Processed: (48744, 80)\n",
      "Bureau Aggregated: (305811, 56)\n",
      "Bureau Balance Aggregated with Curr Final: (134542, 63)\n",
      "POS Cash Balance Aggregated: (337252, 36)\n",
      "Credit Card Balance Aggregated: (103558, 99)\n",
      "Previous Application Aggregated: (338857, 109)\n",
      "Installments Payments Aggregated: (339587, 62)\n"
     ]
    }
   ],
   "source": [
    "# Quick check of shapes\n",
    "print(\"Dataset Shapes:\")\n",
    "print(f\"Application Test Processed: {df_application_test_processed.shape}\")\n",
    "print(f\"Bureau Aggregated: {df_bureau_aggregated.shape}\")\n",
    "print(f\"Bureau Balance Aggregated with Curr Final: {df_bureau_balance_aggregated_with_curr_final.shape}\")\n",
    "print(f\"POS Cash Balance Aggregated: {df_pos_cash_balance_aggregated.shape}\")\n",
    "print(f\"Credit Card Balance Aggregated: {df_credit_card_balance_aggregated.shape}\")\n",
    "print(f\"Previous Application Aggregated: {df_previous_application_aggregated.shape}\")\n",
    "print(f\"Installments Payments Aggregated: {df_installments_payments_aggregated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fe0f22",
   "metadata": {
    "papermill": {
     "duration": 0.006104,
     "end_time": "2025-03-07T20:46:05.956985",
     "exception": false,
     "start_time": "2025-03-07T20:46:05.950881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.5 Final Check: ID Columns in Merged Datasets\n",
    "- This check ensures that **no unwanted ID-related columns** (e.g., system-generated IDs) accidentally sneak into the training dataset.\n",
    "- **`SK_ID_CURR` is the primary key** used to merge datasets and should not be removed.\n",
    "- **`DAYS_ID_PUBLISH` is an important time-related feature** and should not be excluded.\n",
    "- If additional unexpected ID columns appear, they should be investigated and removed if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93058ccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:05.970919Z",
     "iopub.status.busy": "2025-03-07T20:46:05.970545Z",
     "iopub.status.idle": "2025-03-07T20:46:05.975251Z",
     "shell.execute_reply": "2025-03-07T20:46:05.974261Z"
    },
    "papermill": {
     "duration": 0.013795,
     "end_time": "2025-03-07T20:46:05.977067",
     "exception": false,
     "start_time": "2025-03-07T20:46:05.963272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "datasets = {\n",
    "    \"Application Test\": df_application_test_processed,\n",
    "    \"Bureau Aggregated\": df_bureau_aggregated,\n",
    "    \"Bureau Balance Aggregated with Curr Final\": df_bureau_balance_aggregated_with_curr_final,\n",
    "    \"POS Cash Balance Aggregated\": df_pos_cash_balance_aggregated,\n",
    "    \"Credit Card Balance Aggregated\": df_credit_card_balance_aggregated,\n",
    "    \"Previous Application Aggregated\": df_previous_application_aggregated,\n",
    "    \"Installments Payments Aggregated\": df_installments_payments_aggregated,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c26b04",
   "metadata": {
    "papermill": {
     "duration": 0.006398,
     "end_time": "2025-03-07T20:46:05.989949",
     "exception": false,
     "start_time": "2025-03-07T20:46:05.983551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.6 Display Summary of ID Columns in Merged Datasets  \n",
    "This table lists the ID-related columns found in each dataset.  \n",
    "It helps verify that only the correct primary key (`SK_ID_CURR`) is present and that no unnecessary ID columns remain.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3ac343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:06.006463Z",
     "iopub.status.busy": "2025-03-07T20:46:06.006077Z",
     "iopub.status.idle": "2025-03-07T20:46:06.027554Z",
     "shell.execute_reply": "2025-03-07T20:46:06.026003Z"
    },
    "papermill": {
     "duration": 0.032889,
     "end_time": "2025-03-07T20:46:06.029244",
     "exception": false,
     "start_time": "2025-03-07T20:46:05.996355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ID Column Summary:\n",
      "                                  Dataset                  ID Columns\n",
      "                         Application Test SK_ID_CURR, DAYS_ID_PUBLISH\n",
      "                        Bureau Aggregated                  SK_ID_CURR\n",
      "Bureau Balance Aggregated with Curr Final                  SK_ID_CURR\n",
      "              POS Cash Balance Aggregated                  SK_ID_CURR\n",
      "           Credit Card Balance Aggregated                  SK_ID_CURR\n",
      "          Previous Application Aggregated                  SK_ID_CURR\n",
      "         Installments Payments Aggregated                  SK_ID_CURR\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store results\n",
    "sk_id_summary_list = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    # Get all column names\n",
    "    all_columns = df.columns.tolist()\n",
    "    \n",
    "    # Extract columns that contain \"_id_\" (case-insensitive search)\n",
    "    id_cols = [col for col in all_columns if \"_id_\" in col.lower()]\n",
    "\n",
    "    # Append results\n",
    "    sk_id_summary_list.append({\n",
    "        \"Dataset\": name,\n",
    "        \"ID Columns\": \", \".join(id_cols) if id_cols else \"None\"\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "sk_id_summary = pd.DataFrame(sk_id_summary_list)\n",
    "\n",
    "# Display results\n",
    "print(\"Updated ID Column Summary:\")\n",
    "print(sk_id_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3993bd8",
   "metadata": {
    "papermill": {
     "duration": 0.006255,
     "end_time": "2025-03-07T20:46:06.042109",
     "exception": false,
     "start_time": "2025-03-07T20:46:06.035854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Observation  \n",
    "- The **primary key `SK_ID_CURR`** is correctly present in all datasets.  \n",
    "- The **`DAYS_ID_PUBLISH`** column appears in `application_test_processed`, but this is a time-related feature, not an unwanted ID column.  \n",
    "- **No unexpected ID columns were found, confirming that data merging has been done correctly without introducing data leakage.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713697b2",
   "metadata": {
    "papermill": {
     "duration": 0.005955,
     "end_time": "2025-03-07T20:46:06.054398",
     "exception": false,
     "start_time": "2025-03-07T20:46:06.048443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Merging Datasets  \n",
    "Combine the processed secondary datasets with the main **application test dataset**.  \n",
    "- All datasets are merged on **`SK_ID_CURR`**, the unique identifier for loan applications.  \n",
    "- **Left joins (`how='left'`)** ensure that all records from the test dataset are retained, while missing values from secondary datasets are handled appropriately.  \n",
    "- This merged dataset will be used for **making final predictions**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9293e495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:06.068715Z",
     "iopub.status.busy": "2025-03-07T20:46:06.068249Z",
     "iopub.status.idle": "2025-03-07T20:46:07.326362Z",
     "shell.execute_reply": "2025-03-07T20:46:07.324991Z"
    },
    "papermill": {
     "duration": 1.267508,
     "end_time": "2025-03-07T20:46:07.328346",
     "exception": false,
     "start_time": "2025-03-07T20:46:06.060838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final df_final_test shape: (48744, 499)\n"
     ]
    }
   ],
   "source": [
    "df_final_test = df_application_test_processed.merge(df_bureau_aggregated, on='SK_ID_CURR', how='left') \\\n",
    "                                             .merge(df_bureau_balance_aggregated_with_curr_final, on='SK_ID_CURR', how='left') \\\n",
    "                                             .merge(df_pos_cash_balance_aggregated, on='SK_ID_CURR', how='left') \\\n",
    "                                             .merge(df_credit_card_balance_aggregated, on='SK_ID_CURR', how='left') \\\n",
    "                                             .merge(df_previous_application_aggregated, on='SK_ID_CURR', how='left') \\\n",
    "                                             .merge(df_installments_payments_aggregated, on='SK_ID_CURR', how='left')\n",
    "\n",
    "print(f\"‚úÖ Final df_final_test shape: {df_final_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10d0e1",
   "metadata": {
    "papermill": {
     "duration": 0.008203,
     "end_time": "2025-03-07T20:46:07.343609",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.335406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Check Train and Test Dataset Feature Alignment  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be00ca",
   "metadata": {
    "papermill": {
     "duration": 0.006266,
     "end_time": "2025-03-07T20:46:07.356369",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.350103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.1 Load Feature Metadata  \n",
    "Load the stored **feature names, data types, and category mappings** from the training phase.  \n",
    "This ensures that the test dataset is processed consistently with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d329c294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:07.371143Z",
     "iopub.status.busy": "2025-03-07T20:46:07.370736Z",
     "iopub.status.idle": "2025-03-07T20:46:07.394199Z",
     "shell.execute_reply": "2025-03-07T20:46:07.392803Z"
    },
    "papermill": {
     "duration": 0.033132,
     "end_time": "2025-03-07T20:46:07.396215",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.363083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded df_final features, dtypes & category mappings successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load feature names, dtypes & category mappings from JSON\n",
    "json_path = \"/kaggle/input/home-credit-processed-data-and-model/df_final_features.json\"\n",
    "\n",
    "# Load feature names, dtypes & category mappings from JSON\n",
    "with open(json_path, \"r\") as f:\n",
    "    train_metadata = json.load(f)\n",
    "\n",
    "\n",
    "train_features = train_metadata[\"features\"]\n",
    "train_dtypes = {k: pd.api.types.pandas_dtype(v) for k, v in train_metadata[\"dtypes\"].items()}\n",
    "category_mappings = train_metadata.get(\"category_mappings\", {})  # Load category mappings\n",
    "\n",
    "print(\"‚úÖ Loaded df_final features, dtypes & category mappings successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c9ed1",
   "metadata": {
    "papermill": {
     "duration": 0.006489,
     "end_time": "2025-03-07T20:46:07.409435",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.402946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.2 Compare Feature Names and Order  \n",
    "Check if the test dataset has the same feature names and order as the training dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07a1417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:07.423760Z",
     "iopub.status.busy": "2025-03-07T20:46:07.423260Z",
     "iopub.status.idle": "2025-03-07T20:46:07.430432Z",
     "shell.execute_reply": "2025-03-07T20:46:07.429274Z"
    },
    "papermill": {
     "duration": 0.016448,
     "end_time": "2025-03-07T20:46:07.432363",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.415915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Feature names/order mismatch!\n",
      "Missing in Test: set()\n",
      "Extra in Test: {'SK_ID_CURR'}\n"
     ]
    }
   ],
   "source": [
    "# Compare feature names and order\n",
    "test_features = df_final_test.columns.tolist()\n",
    "\n",
    "if train_features == test_features:\n",
    "    print(\"‚úÖ Feature names and order match!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Feature names/order mismatch!\")\n",
    "    print(\"Missing in Test:\", set(train_features) - set(test_features))\n",
    "    print(\"Extra in Test:\", set(test_features) - set(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df90b8b",
   "metadata": {
    "papermill": {
     "duration": 0.0065,
     "end_time": "2025-03-07T20:46:07.445716",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.439216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Observation  \n",
    "- The test dataset contains **one extra feature (`SK_ID_CURR`)** compared to the final feature list from training.  \n",
    "- This happens because `SK_ID_CURR` was **dropped in the training phase before saving the final feature list** in JSON.  \n",
    "- The `TARGET` column does not appear in the mismatch because it was never part of the test dataset.  \n",
    "- This difference is expected and does not affect predictions, as `SK_ID_CURR` is only used for merging and identification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cddfae",
   "metadata": {
    "papermill": {
     "duration": 0.006474,
     "end_time": "2025-03-07T20:46:07.458956",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.452482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.3 Align Data Types  \n",
    "Ensure that all columns in the test dataset have the **same data types** as in the training dataset.  \n",
    "- **Categorical columns** are cast to match the training set categories.  \n",
    "- **Integer vs. Float columns** are adjusted where necessary.  \n",
    "- This step ensures consistency in model input formatting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f889c604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:07.474286Z",
     "iopub.status.busy": "2025-03-07T20:46:07.473906Z",
     "iopub.status.idle": "2025-03-07T20:46:07.676595Z",
     "shell.execute_reply": "2025-03-07T20:46:07.675455Z"
    },
    "papermill": {
     "duration": 0.212796,
     "end_time": "2025-03-07T20:46:07.678593",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.465797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explicitly recast each column using the training metadata\n",
    "for col in train_features:\n",
    "    if col in df_final_test.columns:\n",
    "        desired_dtype = train_dtypes[col]\n",
    "        \n",
    "        # Handle categorical columns\n",
    "        if str(desired_dtype) == 'category':\n",
    "            cats = category_mappings.get(col, None)\n",
    "            if cats is not None:\n",
    "                # Force the column to have exactly these categories (assuming unordered; if ordered, add ordered=True)\n",
    "                cat_dtype = pd.CategoricalDtype(categories=cats, ordered=False)\n",
    "                df_final_test[col] = df_final_test[col].astype(cat_dtype)\n",
    "            else:\n",
    "                df_final_test[col] = df_final_test[col].astype('category')\n",
    "                \n",
    "        # Handle special case: training column is nullable integer (Int64) but test column is float64\n",
    "        elif desired_dtype.name == \"Int64\" and df_final_test[col].dtype == \"float64\":\n",
    "            # Convert from float64 to pandas' nullable integer type\n",
    "            df_final_test[col] = df_final_test[col].astype(\"Int64\")\n",
    "            \n",
    "        else:\n",
    "            # For non-categorical columns, cast directly\n",
    "            df_final_test[col] = df_final_test[col].astype(desired_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d29ce",
   "metadata": {
    "papermill": {
     "duration": 0.006395,
     "end_time": "2025-03-07T20:46:07.691880",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.685485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.4 Verify Data Type Consistency\n",
    "This check ensures that:\n",
    "- All columns in the test dataset match the data types from training.\n",
    "- Categorical columns retain their expected category values.\n",
    "- Integer and float columns are correctly aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b40b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:07.706699Z",
     "iopub.status.busy": "2025-03-07T20:46:07.706257Z",
     "iopub.status.idle": "2025-03-07T20:46:07.733191Z",
     "shell.execute_reply": "2025-03-07T20:46:07.732162Z"
    },
    "papermill": {
     "duration": 0.03657,
     "end_time": "2025-03-07T20:46:07.735022",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.698452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All data types match!\n"
     ]
    }
   ],
   "source": [
    "# Compare category lists\n",
    "dtype_mismatches = {}\n",
    "\n",
    "for col in train_features:\n",
    "    dt_train = train_dtypes[col]\n",
    "    dt_test = df_final_test[col].dtype\n",
    "\n",
    "    # If both training and test dtypes are categorical, compare their category lists\n",
    "    if str(dt_train) == 'category' and str(dt_test) == 'category':\n",
    "        cats_train = category_mappings.get(col, [])\n",
    "        cats_test = list(df_final_test[col].cat.categories)\n",
    "        if cats_train != cats_test:\n",
    "            dtype_mismatches[col] = (f\"categories={cats_train}\", f\"categories={cats_test}\")\n",
    "    else:\n",
    "        if dt_train != dt_test:\n",
    "            dtype_mismatches[col] = (dt_train, dt_test)\n",
    "\n",
    "if not dtype_mismatches:\n",
    "    print(\"‚úÖ All data types match!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data Type Mismatches:\")\n",
    "    for col, (train_dtype, test_dtype) in dtype_mismatches.items():\n",
    "        print(f\"‚ùå {col}: Train = {train_dtype}, Test = {test_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a858ea",
   "metadata": {
    "papermill": {
     "duration": 0.006278,
     "end_time": "2025-03-07T20:46:07.748101",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.741823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a73d1",
   "metadata": {
    "papermill": {
     "duration": 0.006174,
     "end_time": "2025-03-07T20:46:07.760997",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.754823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1 Load Trained Model\n",
    "Load the trained LightGBM model to generate predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21737e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:07.775262Z",
     "iopub.status.busy": "2025-03-07T20:46:07.774885Z",
     "iopub.status.idle": "2025-03-07T20:46:07.917387Z",
     "shell.execute_reply": "2025-03-07T20:46:07.916044Z"
    },
    "papermill": {
     "duration": 0.152224,
     "end_time": "2025-03-07T20:46:07.919679",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.767455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LightGBM model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the model using LightGBM's native method\n",
    "model_path = \"/kaggle/input/home-credit-processed-data-and-model/lightgbm_model.txt\"\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "print(\"‚úÖ LightGBM model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae75a2",
   "metadata": {
    "papermill": {
     "duration": 0.006476,
     "end_time": "2025-03-07T20:46:07.932967",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.926491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2 Generate Predictions and Save Output File\n",
    "- Apply the trained LightGBM model to the processed test dataset.\n",
    "- Save the final predictions as a structured CSV file for analysis or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd292295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:46:07.948015Z",
     "iopub.status.busy": "2025-03-07T20:46:07.947654Z",
     "iopub.status.idle": "2025-03-07T20:46:12.244623Z",
     "shell.execute_reply": "2025-03-07T20:46:12.243355Z"
    },
    "papermill": {
     "duration": 4.306638,
     "end_time": "2025-03-07T20:46:12.246404",
     "exception": false,
     "start_time": "2025-03-07T20:46:07.939766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions complete. Prediction file saved as 'predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "# 2. Prepare your test dataset using the same features as in training\n",
    "X_test = df_final_test[train_features]\n",
    "\n",
    "# 3. Run predictions on the test dataset\n",
    "#    For LightGBM, model.predict returns probabilities for binary classification.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 4. Create a prediction DataFrame and save to CSV.\n",
    "prediction_output = pd.DataFrame({\n",
    "    \"SK_ID_CURR\": df_final_test[\"SK_ID_CURR\"],\n",
    "    \"TARGET\": predictions.round(1)\n",
    "})\n",
    "\n",
    "prediction_output.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Predictions complete. Prediction file saved as 'predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1eeac2",
   "metadata": {
    "papermill": {
     "duration": 0.007176,
     "end_time": "2025-03-07T20:46:12.260867",
     "exception": false,
     "start_time": "2025-03-07T20:46:12.253691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ‚úÖ Final Summary\n",
    "- The trained LightGBM model was successfully applied to the test dataset.\n",
    "- Predictions were generated and saved as **`predictions.csv`**.\n",
    "- While real-time inference is preferred in production (as demonstrated in my [Angular/Flask-based deployed app](https://ai.fullstackista.com/ai-loan-default-predictor/)), structured output files like this are useful for batch processing and reporting."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6775251,
     "sourceId": 10901361,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.304089,
   "end_time": "2025-03-07T20:46:13.390307",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-07T20:45:51.086218",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
